# slides_script.md

## Story Outline（章立て：7項目）

1) **導入**: 高信頼ソフトウェアにおける仕様駆動開発の重要性と課題
2) **問題提起**: 自然言語仕様と実装の乖離問題
3) **既存手法の限界**: 差分テストの「Semantic Blind Spot」
4) **提案手法**: SpecAudit - 仕様チェックリスト駆動監査フレームワーク
5) **評価**: Ethereum Fusakaアップグレード監査コンテストでの検証
6) **結果と限界**: 戦略別寄与、FP原因分析、High/Medium未検出の分析
7) **結論と今後**: 得られた知見と改善の方向性

---

## Slide-by-slide Script

### Slide 1: SpecAudit - 仕様チェックリスト駆動による多実装コンプライアンス監査
- On-slide (what to show):
  - タイトル: 「SpecAudit: 仕様チェックリスト駆動による多実装コンプライアンス監査」
  - サブタイトル: Specification-Checklist-Driven Auditing for Multi-Implementation Compliance
  - 著者名・所属
  - 発表日
  - 背景に仕様書とコードが繋がるイメージ（抽象的なライン図）
- Speaker script (what to say):
  本日は「SpecAudit」という、仕様チェックリスト駆動による多実装コンプライアンス監査フレームワークについて発表します。本研究では、自然言語で書かれた仕様と、それを実装した複数のコードベースの間の整合性を、LLMエージェントを用いて体系的に監査する手法を提案し、実際のセキュリティ監査コンテストで評価しました。
- Transition: まず、この研究の背景となる課題についてご説明します。

---

### Slide 2: 高信頼ソフトウェアでは仕様と実装の整合性確認が不可欠である
- On-slide (what to show):
  - 左側: 自然言語仕様の例（RFC、EIP、設計書のイメージ）
  - 右側: 複数実装のコードベース（異なる言語で書かれた複数クライアント）
  - 中央: 「整合性監査」を示す双方向矢印と「?」マーク
  - キーワード: 「MUST」「SHOULD」「MAY」（RFC 2119の規範語）
- Speaker script (what to say):
  航空管制、医療機器、金融システム、そしてブロックチェーンなど、高信頼性が求められるソフトウェアでは、仕様と実装の整合性確認が極めて重要です。これらのシステムでは、RFCや技術仕様書といった自然言語で記述された仕様があり、「MUST」「SHOULD」といった規範語で要件が定義されています。しかし、これらの仕様を実装に正しく反映できているかを確認する作業は、現在も多くが人手に依存しており、時間がかかり、エラーが入りやすいという問題があります。
- Transition: 特に、複数のチームが同じ仕様を独立に実装する環境では、さらに複雑な問題が生じます。

---

### Slide 3: 多実装環境では「全員が同じ誤解をする」問題が発生する
- On-slide (what to show):
  - 上部: 1つの仕様文書（曖昧な記述を含む）
  - 下部: 複数の実装チーム（A, B, C, D...）が同じ方向に矢印で繋がる
  - 全実装が同じ「誤った解釈」に到達している図解
  - Knight & Leveson (1986) の引用: 「独立実装でも共通障害は起きる」
- Speaker script (what to say):
  多実装環境、つまり同じ仕様を複数のチームが独立に実装する環境では、興味深い問題が生じます。1986年のKnightとLevesonの有名な研究で示されたように、独立に開発された実装であっても、仕様の曖昧な部分について同じ誤解をする「共通モード障害」が偶然より高い頻度で発生します。これは、仕様文書の曖昧さや、暗黙の前提が原因です。
- Transition: この問題に対して、既存の自動化手法はどのように対処してきたでしょうか。

---

### Slide 4: 差分テストは「全実装が同じ誤りをする」と検知できない
- On-slide (what to show):
  - 差分テスト/差分ファジングの概念図（複数実装に同じ入力、出力を比較）
  - 「挙動が一致」→「バグ検知不可」のフロー
  - 赤字で「Semantic Blind Spot」
  - 代表的手法: Fluffy (NDSS 2021), LOKI (NDSS 2023)
- Speaker script (what to say):
  既存の自動化手法として、差分テストや差分ファジングがあります。FluffyやLOKIといった手法は、複数の実装に同じ入力を与え、出力の「差分」を検知することでバグを発見します。これは非常に強力な手法ですが、構造的な限界があります。それは、すべての実装が同じ誤解をしている場合、差分が出ないためバグを検知できないという点です。我々はこれを「Semantic Blind Spot」、意味的盲点と呼んでいます。
- Transition: この盲点を突破するには、「差分」ではなく「仕様そのもの」を根拠にした監査が必要です。

---

### Slide 5: 仕様チェックリストは差分に依存しない監査の根拠となる
- On-slide (what to show):
  - 中央: 「仕様チェックリスト」（チェック項目のリスト）
  - 左矢印: 仕様文書から抽出
  - 右矢印: 各実装に対して独立に適用
  - 「差分ではなく、仕様を根拠に正しさを判定」というメッセージ
- Speaker script (what to say):
  本研究の核心的なアイデアは、仕様からチェックリストを抽出し、それを各実装に独立に適用することです。これにより、実装間の差分ではなく、仕様そのものを正しさの根拠とできます。さらに重要なのは、ある実装で発見したバグパターンを、同じチェックリストを用いて他の実装にも横展開できる点です。これにより、1つの発見を多実装環境全体に効率的にスケールできます。
- Transition: この考え方を具体化したフレームワークがSpecAuditです。

---

### Slide 6: SpecAuditは2フェーズ・3戦略で仕様準拠を体系的に監査する
- On-slide (what to show):
  - アーキテクチャ図（2段構成）
    - Phase 1: 知識構造化（仕様抽出 → 実装マッピング → パターンDB）
    - Phase 2: 体系的監査（Strategy A/B/C）
  - 出力: 監査レポート（proof_trace付き）
  - 各Phaseをボックスで表現し、矢印でフローを示す
- Speaker script (what to say):
  SpecAuditは2つのフェーズで構成されます。Phase 1は知識構造化で、自然言語仕様から規範要件を抽出し、それをコードにマッピングし、既知の脆弱性パターンをデータベース化します。Phase 2は体系的監査で、3つの戦略を適用します。各戦略については次のスライドで詳しく説明します。最終的に、証跡付きの監査レポートを生成し、人間による検証を支援します。
- Transition: 3つの監査戦略について詳しく見ていきましょう。

---

### Slide 7: 監査戦略Bは1つの発見を多実装に横展開しスケールさせる
- On-slide (what to show):
  - 3戦略の比較表:
    - Strategy A: 仕様ベース静的監査（単一実装の直接チェック）
    - Strategy B: Cross-Implementation Checks（横展開）← 強調
    - Strategy C: 動的テスト生成
  - Strategy Bの図解: Client Aで発見 → Client B, C, D... へ展開
  - 具体例: 「Range check欠落」をClient A→B, Cへ横展開
- Speaker script (what to say):
  3つの戦略のうち、特に重要なのがStrategy B、Cross-Implementation Checksです。これは、ある実装で発見したバグパターンを抽象化し、同じチェックリスト項目を他のすべての実装に対して検証する戦略です。例えば、Client Aで「Range checkの欠落」を発見した場合、同じチェック項目をClient B、C、Dにも適用します。多実装環境では、仕様の曖昧な部分で複数チームが同じ誤解をしている可能性が高いため、この横展開が非常に効果的です。
- Transition: この手法を実際のセキュリティ監査コンテストで評価しました。

---

### Slide 8: Ethereum Fusakaアップグレードの11クライアント監査で評価した
- On-slide (what to show):
  - 評価環境の概要表:
    - 対象: Ethereum Fusakaアップグレード
    - クライアント数: 11（CL 6 + EL 5）
    - 全参加者: 366提出、101 valid (27.6%)
    - 我々: 54提出、17 valid (31.5%)
  - クライアント名リスト（Nimbus, Lighthouse, Prysm, Teku, Lodestar, Grandine / Geth, Reth, Erigon, Nethermind, Besu）
- Speaker script (what to say):
  評価環境として、Ethereumの次期アップグレードであるFusakaの監査コンテストを使用しました。このコンテストでは、コンセンサスレイヤー6つ、実行レイヤー5つ、計11の本番クライアントが対象でした。全参加者合計で366件の提出があり、101件が有効と判定され、全体の有効率は27.6%でした。我々は54件を提出し、17件が有効と認められ、有効率31.5%で全体平均を上回りました。
- Transition: 最も重要な結果として、戦略別の寄与分析をご紹介します。

---

### Slide 9: Cross-Implementation Checksが有効発見の76.5%を占めた
- On-slide (what to show):
  - 円グラフ: 戦略別寄与（Cross-Impl: 76.5%, Static: 17.6%, Dynamic: 5.9%）
  - 表形式でも併記:
    - Cross-Implementation Checks: 13件 (76.5%)
    - 仕様ベース静的監査: 3件 (17.6%)
    - 動的テスト: 1件 (5.9%)
  - 「1→N スケール」を強調するアイコン
- Speaker script (what to say):
  戦略別の寄与分析の結果、Cross-Implementation Checks、つまり横展開戦略が有効発見の76.5%、17件中13件を占めました。これは、多実装環境において、1つの発見を他の実装に横展開する戦略が極めて有効であることを示しています。仕様の曖昧な部分で複数チームが同じ誤解をしているという仮説と整合する結果です。一方、動的テストは1件のみで、5.9%にとどまりました。
- Transition: 次に、誤検知の原因分析を見ていきます。

---

### Slide 10: 誤検知の最大原因はLLM能力ではなく脅威モデル不整合である
- On-slide (what to show):
  - 棒グラフ: FP原因分布
    - 脅威モデル不整合: 21件 (56.8%) ← 最大、強調
    - 既知/重複: 8件 (21.6%)
    - 分析エラー: 5件 (13.5%)
    - スコープ外: 3件 (8.1%)
  - 具体例: 「ELを信頼しない」vs「ELは信頼コンポーネント」の対比
- Speaker script (what to say):
  37件の誤検知を分析した結果、最大の原因は「脅威モデル不整合」で56.8%を占めました。具体的には、我々のプロンプトでは実行レイヤーを「信頼しない」と設定していましたが、コンテストのルールでは「信頼コンポーネント」として扱われていました。この前提条件のズレが21件の誤検知を生みました。重要な洞察は、誤検知の過半数がLLMの推論能力の問題ではなく、前提条件の不整合に起因しているという点です。これは、工程として前提を明示化することで改善可能です。
- Transition: しかし、本手法には明確な限界もあります。

---

### Slide 11: High/Medium重大度の脆弱性は検出できなかった
- On-slide (what to show):
  - 重大度別検出率の表:
    - High: 0/5 (0%)
    - Medium: 0/2 (0%)
    - Low: 1/8 (12.5%)
  - 見逃し原因の内訳（円グラフまたはバー）:
    - 複雑な状態遷移: 42.9%
    - 動的境界条件: 28.6%
    - 外部依存: 14.3%
    - 仕様ニュアンス: 14.3%
  - 赤字で「Critical Limitation」
- Speaker script (what to say):
  重要な限界として、High重大度5件、Medium重大度2件、計7件の重大な脆弱性をすべて見逃しました。見逃しの原因を分析すると、42.9%が「複雑な状態遷移」、28.6%が「動的境界条件」でした。例えば、長い状態遷移チェーンの先にある脆弱性や、実行時に決まる境界値の問題は、静的なチェックリスト監査では捕捉できません。これは本手法の根本的な限界であり、モデル検査やシンボリック実行との統合が必要な領域です。
- Transition: これらの結果から、手法の得意・不得意が明確になりました。

---

### Slide 12: 本手法は横展開に強く複雑な状態空間探索に弱い
- On-slide (what to show):
  - 2列構成:
    - 左列（緑）「得意」:
      - 規範要件（MUST/SHOULD）の直接検証
      - 多実装への横展開（1→N）
      - 証跡付き監査レポート生成
    - 右列（赤）「苦手」:
      - 複雑な状態遷移（深い状態到達性）
      - 動的境界条件（実行時決定）
      - 外部依存・仕様の微妙なニュアンス
  - 改善方向の矢印: モデル検査、ファジング、仕様形式化
- Speaker script (what to say):
  結果をまとめると、本手法は規範要件の直接検証と、発見の多実装への横展開に強みがあります。仕様から抽出したチェックリストにより、監査を再現可能な工程として固定でき、証跡も残せます。一方、複雑な状態遷移を伴う脆弱性や、実行時に決まる境界条件の検出は苦手です。これらの領域には、モデル検査、シンボリック実行、プロパティベーステストといった補完的な手法が必要です。
- Transition: 関連研究との比較で、本手法の位置づけを確認します。

---

### Slide 13: 本手法は仕様起点×多実装×脅威モデル形式化で差別化される
- On-slide (what to show):
  - ポジショニングマップ（2軸: 仕様起点 vs コード起点、単一実装 vs 多実装）
  - 比較対象:
    - RFCAudit: 仕様起点、単一実装
    - RepoAudit: コード起点、単一実装（validator重視）
    - Fluffy/LOKI: コード起点、多実装（差分依存）
    - SpecAudit: 仕様起点、多実装（チェックリスト再利用）
  - 「脅威モデル形式化」を独自要素として強調
- Speaker script (what to say):
  関連研究と比較すると、RFCAuditは仕様起点ですが単一実装を対象としています。RepoAuditはコード起点で検証器によるFP削減を重視します。FluffyやLOKIは多実装の差分を利用しますが、仕様への言及はありません。SpecAuditは、仕様起点で多実装に対応し、かつ脅威モデルを明示的に形式化する点で差別化されます。特に、FPの最大原因が脅威モデル不整合であるという知見に基づき、前提条件を工程として固定することを重視しています。
- Transition: 最後に、本研究の貢献と今後の展望をまとめます。

---

### Slide 14: 仕様チェックリスト化と前提の形式化が監査をスケールさせる
- On-slide (what to show):
  - 3つの貢献（番号付きリスト）:
    1. Cross-Implementation Checksが有効発見の76.5%を占めた
    2. FPの56.8%が脅威モデル不整合に起因（改善可能）
    3. High/Medium未検出の原因は複雑状態遷移・動的境界（限界）
  - Take-home message: 「監査のボトルネックはモデルの賢さだけでなく、前提・境界・規範要件の固定にある」
  - 今後の方向: モデル検査との統合、プロパティベーステスト、仕様の形式化
- Speaker script (what to say):
  本研究の貢献をまとめます。第一に、多実装環境において、Cross-Implementation Checksが有効発見の76.5%を占め、チェックリスト再利用による横展開の有効性を示しました。第二に、誤検知の56.8%が脅威モデル不整合に起因しており、LLMの能力向上だけでなく、前提条件の形式化が重要であることを示しました。第三に、High/Medium脆弱性の未検出という限界を正直に報告し、その原因を分析しました。監査のボトルネックは「モデルを賢くする」だけでなく、「前提・境界・規範要件を工程として固定する」ことにあると考えています。
- Transition: 以上で発表を終わります。ご質問をお待ちしております。

---

### Slide 15: ご清聴ありがとうございました
- On-slide (what to show):
  - タイトル: 「ご清聴ありがとうございました」
  - 連絡先・所属
  - QRコード（論文/コードへのリンク、任意）
  - 主要数値の再掲（小さく）:
    - Cross-Impl: 76.5%
    - FP脅威モデル: 56.8%
    - High/Medium: 0%
- Speaker script (what to say):
  ご清聴ありがとうございました。SpecAuditは、仕様チェックリストを用いて多実装の整合性を監査するフレームワークです。実運用環境での評価により、横展開戦略の有効性と、脅威モデル形式化の重要性を示しました。ご質問やご意見をお待ちしております。
- Transition: （Q&Aへ）

---

## Backup Slides（4枚）

### Backup 1: 具体的な発見事例 - Nimbus custody rotation DoS
- On-slide:
  - 発見のフロー図:
    1. 仕様: 「custody_group_countはNUMBER_OF_CUSTODY_GROUPS (128) を超えてはならない」
    2. マッピング: peerdas_helpers.nim
    3. 逸脱: ループ条件で検証なし（`while custody_groups.len < custody_group_count`）
  - コード断片（該当箇所をハイライト）
  - 結果: Low重大度として認定
- Speaker script:
  唯一のLow重大度発見の具体例です。仕様から「custody_group_countは128を超えてはならない」という要件を抽出し、Nimbusの該当コードにマッピングしました。実装では、この上限チェックがループ条件で行われておらず、悪意あるピアが過大な値を送ることでDoS攻撃が可能でした。これは仕様駆動監査の成功例です。

### Backup 2: V2改善 - TRUSTMODELによる脅威モデル形式化
- On-slide:
  - V1とV2の比較図:
    - V1: 脅威モデルが暗黙的（プロンプト内に散在）
    - V2: TRUSTMODELを独立フェーズに（アクター、信頼レベル、境界エッジ）
  - V2アーティファクト例:
    - 8アクター、12境界エッジ
    - 信頼レベル: TRUSTED / SEMI_TRUSTED / UNTRUSTED
  - 期待される効果: 脅威モデル不整合によるFPの削減
- Speaker script:
  V1の分析に基づき、V2では脅威モデルを独立したフェーズとして形式化しました。アクターと信頼レベル、境界エッジを明示的に定義することで、監査前に前提条件を固定します。これにより、脅威モデル不整合によるFPを構造的に削減することを目指しています。V2は設計提案であり、実証評価は今後の課題です。

### Backup 3: 見逃したHigh重大度脆弱性の詳細分析
- On-slide:
  - 5件のHigh脆弱性リスト:
    - #40: Proposer計算エラー（仕様ニュアンス）
    - #190: Prysm キャッシュ問題（複雑状態遷移）
    - #203: c-kzg弱いFiat-Shamir（外部依存）
    - #176: Nethermind malformed blob tx（動的境界）
    - #210: Nethermind blob hash不一致（動的境界）
  - 各脆弱性の見逃し原因カテゴリを色分け
- Speaker script:
  見逃した5件のHigh脆弱性を詳細に分析しました。#40は仕様の微妙なニュアンス、#190は複雑なキャッシュ状態遷移、#203は外部暗号ライブラリの問題、#176と#210は実行時の動的境界条件に関わるものでした。これらは静的なチェックリスト監査の限界を示しており、異なるアプローチが必要な領域です。

### Backup 4: 評価環境の詳細 - Fusaka監査コンテストの重大度定義
- On-slide:
  - 重大度閾値の表:
    - Critical: 50%超のネットワーク影響
    - High: 33%超
    - Medium: 5%超
    - Low: 0.01%超
    - Informational: クライアント側が修正価値を認めた場合のみ有効
  - 賞金プール: $2,000,000（うちInfo pool: $25,000）
  - Low/Infoでも「運用上の価値がある」というメッセージ
- Speaker script:
  Fusakaコンテストの重大度定義は非常に厳格で、Highはネットワークの33%以上に影響を与える脆弱性に限定されます。このため、Low/Informationalでも、実運用における事故回避や仕様準拠性の担保として価値があります。我々の17件の有効発見のうち16件がInformationalでしたが、これらはすべてクライアント側が修正価値を認めたものです。

---

## 発表時間配分（目安）

| スライド | 内容 | 目安時間 |
|:--------|:-----|:--------|
| 1 | タイトル | 30秒 |
| 2 | 背景：仕様と実装の整合性 | 50秒 |
| 3 | 問題：共通誤解 | 50秒 |
| 4 | Gap：差分テストの限界 | 60秒 |
| 5 | キーアイデア：仕様チェックリスト | 50秒 |
| 6 | 提案：SpecAudit概要 | 50秒 |
| 7 | 戦略B：Cross-Impl Checks | 60秒 |
| 8 | 評価環境：Fusaka | 50秒 |
| 9 | 結果1：戦略別寄与 | 60秒 |
| 10 | 結果2：FP原因分析 | 60秒 |
| 11 | 結果3：限界分析 | 60秒 |
| 12 | 議論：得意・苦手 | 50秒 |
| 13 | 関連研究との位置づけ | 50秒 |
| 14 | 結論・貢献 | 60秒 |
| 15 | Thank you | 20秒 |
| **合計** | | **約14分** |

---

## スライドデザインガイドライン

- **フォント**: BIZ UDPMincho（日本語）
- **背景**: 白
- **文字・図**: 黒基調（アクセントカラーは赤・緑を控えめに）
- **タイトル**: 各スライドは「主張の文章」をタイトルに（名詞句禁止）
- **本文**: 箇条書きは最小限、視覚的証拠（図・表・数値）中心
- **1スライド = 1メッセージ**: 要素は最大3つ
- **タイトル/Thank youスライド**: coverレイアウト
